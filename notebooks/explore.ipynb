{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d130d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path('../datasets_secret')\n",
    "templates_raw = pd.read_csv(dataset_dir / 'templates_w_nodes.csv')\n",
    "\n",
    "\n",
    "# convert content to json\n",
    "templates_raw['content_dict'] = templates_raw['content'].apply(json.loads)\n",
    "\n",
    "templates_raw.head()\n",
    "\n",
    "templates_raw.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_text_filter = lambda x: any(node['type'] == 'text' for node in x['nodes'])\n",
    "any_image_filter = lambda x: any(node['type'] == 'image' for node in x['nodes'])\n",
    "not_any_geo_filter = lambda x: not any(node['type'] == 'geo' for node in x['nodes'])\n",
    "\n",
    "# Filter for templates with just images and text, and NEVER geo\n",
    "templates_images_and_text = templates_raw[\n",
    "    (templates_raw['content_dict'].apply(any_text_filter)\n",
    "    | templates_raw['content_dict'].apply(any_image_filter))\n",
    "    &\n",
    "    templates_raw['content_dict'].apply(not_any_geo_filter)\n",
    "]\n",
    "num_templates = len(templates_images_and_text)\n",
    "print(f\"Number of templates: {num_templates}\")\n",
    "\n",
    "# Print a random template\n",
    "random_template = templates_images_and_text.iloc[np.random.randint(0, num_templates)]\n",
    "print(random_template.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc67a3",
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate UI quality scores for originals and reconstructions\nimport sys\nsys.path.append('..')\nfrom lib.uiclip import score_image\nimport matplotlib.pyplot as plt\n\ncanva_dir = Path('../datasets/canva')\nreconstructions_dir = Path('../datasets/reconstructions')\n\nprint(\"Evaluating UI quality scores...\")\n\n# Get all original Canva images\ncanva_images = sorted(canva_dir.glob('*.webp'))\n\nresults = []\n\nfor img_path in canva_images:\n    design_name = img_path.stem\n    \n    print(f\"Scoring {design_name}...\")\n    \n    # Score original\n    original_score = score_image(img_path, description=\"graphic design\")\n    \n    # Score reconstruction if it exists\n    reconstruction_path = reconstructions_dir / design_name / \"render.png\"\n    if reconstruction_path.exists():\n        reconstruction_score = score_image(reconstruction_path, description=\"graphic design\")\n        \n        results.append({\n            'name': design_name,\n            'original': original_score,\n            'reconstruction': reconstruction_score,\n            'diff': reconstruction_score - original_score\n        })\n\n# Convert to DataFrame\ndf_scores = pd.DataFrame(results)\nprint(f\"\\n{df_scores}\")\n\n# Plot comparison\nif len(df_scores) > 0:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Scatter plot: Original vs Reconstruction\n    ax1 = axes[0]\n    ax1.scatter(df_scores['original'], df_scores['reconstruction'], s=100, alpha=0.6)\n    \n    # Add diagonal line (perfect reconstruction)\n    lims = [\n        min(df_scores['original'].min(), df_scores['reconstruction'].min()) - 0.05,\n        max(df_scores['original'].max(), df_scores['reconstruction'].max()) + 0.05,\n    ]\n    ax1.plot(lims, lims, 'k--', alpha=0.3, label='Perfect reconstruction')\n    \n    ax1.set_xlabel('Original Score', fontsize=12)\n    ax1.set_ylabel('Reconstruction Score', fontsize=12)\n    ax1.set_title('Original vs Reconstruction Aesthetic Scores', fontsize=14)\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    ax1.set_xlim(lims)\n    ax1.set_ylim(lims)\n    \n    # Bar plot: Difference by design\n    ax2 = axes[1]\n    colors = ['green' if d >= 0 else 'red' for d in df_scores['diff']]\n    ax2.bar(range(len(df_scores)), df_scores['diff'], color=colors, alpha=0.6)\n    ax2.axhline(y=0, color='k', linestyle='-', linewidth=0.8)\n    ax2.set_xlabel('Design', fontsize=12)\n    ax2.set_ylabel('Score Difference (Recon - Original)', fontsize=12)\n    ax2.set_title('Aesthetic Score Difference by Design', fontsize=14)\n    ax2.set_xticks(range(len(df_scores)))\n    ax2.set_xticklabels([name[:15] + '...' if len(name) > 15 else name \n                         for name in df_scores['name']], rotation=45, ha='right')\n    ax2.grid(True, alpha=0.3, axis='y')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print summary stats\n    print(\"\\n\" + \"=\"*60)\n    print(\"SUMMARY STATISTICS\")\n    print(\"=\"*60)\n    print(f\"Average Original Score:        {df_scores['original'].mean():.4f}\")\n    print(f\"Average Reconstruction Score:  {df_scores['reconstruction'].mean():.4f}\")\n    print(f\"Average Difference:            {df_scores['diff'].mean():+.4f}\")\n    print(f\"\\nBest Reconstruction:  {df_scores.loc[df_scores['reconstruction'].idxmax(), 'name']}\")\n    print(f\"  Score: {df_scores['reconstruction'].max():.4f}\")\n    print(f\"\\nWorst Reconstruction: {df_scores.loc[df_scores['reconstruction'].idxmin(), 'name']}\")\n    print(f\"  Score: {df_scores['reconstruction'].min():.4f}\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}